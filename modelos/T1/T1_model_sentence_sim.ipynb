{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de Sentence Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculo embeddings frases entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_dict(model, train=train):\n",
    "    # Crea un diccionario para almacenar los embeddings de las frases para cada etiqueta\n",
    "    embeddings_dict = {label: [] for label in range(1, 22)}\n",
    "    # Calcula los embeddings para cada frase y almacénalos en el diccionario\n",
    "    for _, row in train.iterrows():\n",
    "        frase = row['text']\n",
    "        label = row['label']\n",
    "        embedding = model.encode(frase)\n",
    "        embeddings_dict[label].append(embedding)\n",
    "    \n",
    "    return embeddings_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def classify_texts(model,embeddings_dict, classify=classify):\n",
    "    results = pd.DataFrame(columns=['docid', 'text', 'label', 'relevance'])\n",
    "    for _, row in classify.iterrows():\n",
    "        docid = row['docid']\n",
    "        text = row['text']\n",
    "\n",
    "        # Calcula el embedding de la frase\n",
    "        text_embedding = model.encode(text).reshape(1, -1)\n",
    "\n",
    "        # Calcula la similitud con cada síntoma\n",
    "        similarities = {label: cosine_similarity(text_embedding, embeddings).mean() for label, embeddings in embeddings_dict.items()}\n",
    "\n",
    "        # Encuentra el síntoma con la mayor similitud\n",
    "        label, relevance = max(similarities.items(), key=lambda x: x[1])\n",
    "\n",
    "        rescaled_relevance = np.interp(relevance, (0, 1), (0, 10))\n",
    "\n",
    "        # Añade los resultados al DataFrame\n",
    "        new_data = [docid, text, label, rescaled_relevance]\n",
    "        results.loc[len(results)] = new_data\n",
    "        \n",
    "    return results\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "función por batches por si no caben todos los embeddings en memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def classify_texts(model, embeddings_dict, classify = classify, batch_size=10000):\n",
    "    results = pd.DataFrame(columns=['docid', 'text', 'label', 'relevance'])\n",
    "    # Para cada lote de frases en el DataFrame\n",
    "    for i in range(0, len(classify), batch_size):\n",
    "        batch = classify[i:i+batch_size]\n",
    "\n",
    "        # Calcula los embeddings para las frases en el lote\n",
    "        batch_embeddings = model.encode(batch['text'].tolist())\n",
    "\n",
    "        # Para cada frase en el lote\n",
    "        for docid, text, text_embedding in zip(batch['docid'], batch['text'], batch_embeddings):\n",
    "            # Calcula la similitud con cada síntoma\n",
    "            similarities = {label: cosine_similarity(text_embedding.reshape(1, -1), embeddings).mean() for label, embeddings in embeddings_dict.items()}\n",
    "\n",
    "            # Encuentra el síntoma con la mayor similitud\n",
    "            label, relevance = max(similarities.items(), key=lambda x: x[1])\n",
    "\n",
    "            # Escala la relevancia para que esté en el rango de 0 a 10\n",
    "            relevance = np.interp(relevance, (0, 1), (0, 10))\n",
    "\n",
    "            # Añade los resultados al DataFrame\n",
    "            new_data = [docid, text, label, relevance]\n",
    "            results.loc[len(results)] = new_data\n",
    "\n",
    "    return results\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_texts(model, embeddings_dict, classify):\n",
    "    classify = pd.read_csv(classify)\n",
    "    # Calcula los embeddings de todas las frases a la vez\n",
    "    classify['text'] = classify['text'].fillna('')\n",
    "    text_embeddings = model.encode(classify['text'].tolist())\n",
    "\n",
    "    # Calcula la similitud con cada síntoma para cada frase\n",
    "    similarities = np.array([cosine_similarity(text_embeddings, embeddings).mean(axis=1) for label, embeddings in embeddings_dict.items()])\n",
    "\n",
    "    # Encuentra el índice del síntoma con la mayor similitud para cada frase\n",
    "    max_similarity_indices = np.argmax(similarities, axis=0)\n",
    "\n",
    "    # Usa los índices para obtener las etiquetas y las relevancias correspondientes\n",
    "    labels = np.array(list(embeddings_dict.keys()))[max_similarity_indices]\n",
    "    relevances = np.max(similarities, axis=0)\n",
    "\n",
    "    # Reescala las relevancias\n",
    "    rescaled_relevances = np.interp(relevances, (0, 1), (0, 10))\n",
    "\n",
    "    # Crea el DataFrame de resultados\n",
    "    results = pd.DataFrame({\n",
    "        'docid': classify['docid'],\n",
    "        'text': classify['text'],\n",
    "        'label': labels,\n",
    "        'relevance': rescaled_relevances\n",
    "    })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "### all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el modelo pre-entrenado\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = create_embeddings_dict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_texts(model, embeddings_dict).sort_values(by='relevance', ascending=False).to_csv('results_ll-MiniLM-L6-v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all-MiniLM-L12-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = create_embeddings_dict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_texts(model, embeddings_dict).sort_values(by='relevance', ascending=False).to_csv('results_ll-MiniLM-L12-v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all-mpnet-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = create_embeddings_dict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_texts(model, embeddings_dict).sort_values(by='relevance', ascending=False).to_csv('results_ll-mpnet-base-v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas de Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(text, embeddings_dict):\n",
    "\n",
    "    text_embedding = model.encode(text)\n",
    "\n",
    "    similarities = {label: cosine_similarity(text_embedding.reshape(1, -1), embeddings).mean() for label, embeddings in embeddings_dict.items()}\n",
    "    label = max(similarities.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "_ , X_val = train_test_split(train, test_size=0.15, random_state=42, stratify=train['label'])\n",
    "np.random.seed(42)\n",
    "dict_dataset= DatasetDict()\n",
    "dict_dataset['test'] = Dataset.from_pandas(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dataset = dict_dataset.remove_columns(['__index_level_0__', 'length', 'docid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "models = ['sentence-transformers/all-MiniLM-L6-v2', 'sentence-transformers/all-MiniLM-L12-v2', 'sentence-transformers/all-mpnet-base-v2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    model = SentenceTransformer(model)\n",
    "    model.to(device)\n",
    "    embeddings_dict = create_embeddings_dict(model)\n",
    "    y_pred = [get_prediction(text, embeddings_dict) for text in dict_dataset['test']['text']]\n",
    "    y_true = dict_dataset['test']['label']\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    micro_precision = precision_score(y_true, y_pred, average='micro')\n",
    "    micro_recall = recall_score(y_true, y_pred, average='micro')\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    print(f'Micro Precision: {micro_precision}')\n",
    "    print(f'Micro Recall: {micro_recall}')\n",
    "    print(f'Micro F1-Score: {micro_f1}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
