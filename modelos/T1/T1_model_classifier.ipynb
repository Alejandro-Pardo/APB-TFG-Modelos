{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vamos a leer los datos de test y a montarlos en un csv para poder trabajar con ellos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hay caracteres especiales (&) y no imprimibles en el árbol xml, por lo que hay que tratarlos antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def corregir_caracteres_especiales(archivo):\n",
    "    with open(archivo, 'r', encoding='utf8') as entrada:\n",
    "        contenido = '<ROOT>' + entrada.read() + '</ROOT>'\n",
    "    # Reemplaza '&' que no son parte de un XML con '&amp; y los caracteres no imprimibles con ''\n",
    "    contenido_corregido = re.sub(r'&(?!(amp|lt|gt|apos|quot);)', '&amp;', contenido)\n",
    "    contenido_corregido = re.sub(r'[\\x00-\\x1F\\x7F]','', contenido_corregido)\n",
    "    with open(archivo, 'w', encoding='utf8') as salida:\n",
    "        salida.write(contenido_corregido)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def procesar_directorio(directorio):\n",
    "    for nombre_archivo in os.listdir(directorio):\n",
    "        corregir_caracteres_especiales(os.path.join(directorio, nombre_archivo))\n",
    "        \n",
    "procesar_directorio('T1/test/data/')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"directory = 'T1/test/data/'\n",
    "# Recorrer todos los archivos en el directorio\n",
    "for path in os.listdir(directory):\n",
    "    filename = os.path.join(directory, path)\n",
    "    try:\n",
    "        tree = ET.parse(filename)\n",
    "        root = tree.getroot()\n",
    "        for doc in root.findall('DOC'):\n",
    "            docid = doc.find('DOCNO').text\n",
    "            text = str(doc.find('TEXT').text).strip()\n",
    "            new_data= [docid,text]\n",
    "            data_list.append(new_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {filename}: {e}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify = pd.DataFrame(data_list, columns=['docid', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify[\"length\"]=classify['text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify['text'] = classify['text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify.to_csv('classify.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify = pd.read_csv('./classify.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify['length'].describe(percentiles=[0, 0.25, 0.50, 0.75, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "podemos ver como el máxmimo son elementos que por lo general no aportan demasiada información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fila_interes = classify[classify['length'] == 6588]\n",
    "print(fila_interes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relevant = pd.read_csv('T1/train/relevant_texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relevant.drop(columns=['tokens','length_tokens'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relevant = relevant.rename(columns={'length_text':'length', 'symptom':'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relevant.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relevant.to_csv('train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['length'].describe(percentiles=[0, 0.25, 0.50, 0.75, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = train_test_split(train, test_size=0.15, random_state=42, stratify=train['label'])\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dataset= DatasetDict()\n",
    "dict_dataset['train'] = Dataset.from_pandas(X_train )\n",
    "dict_dataset['validation'] = Dataset.from_pandas(X_val)\n",
    "print(dict_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dataset = dict_dataset.remove_columns(['__index_level_0__', 'length', 'docid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SamLowe/roberta-base-go_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"SamLowe/roberta-base-go_emotions\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH= max([len(tokenizer(text).input_ids) for text in dict_dataset['train']['text']])\n",
    "print(\"La longitud máxima de la secuencia es: \", MAX_LENGTH)\n",
    "\n",
    "MAX_LENGTH = min(512, MAX_LENGTH)\n",
    "print(\"max_length\", MAX_LENGTH)\n",
    "\n",
    "tokenizer.model_max_len=MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "  return tokenizer(batch[\"text\"], truncation=True, padding=True, max_length=MAX_LENGTH)\n",
    "\n",
    "encoded_dataset = dict_dataset.map(tokenize, batched=True)\n",
    "encoded_dataset.set_format(\"torch\",columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "import torch\n",
    "import torch.nn as nn       #layes for NN\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "\n",
    "  def __init__(self,checkpoint,num_labels): \n",
    "\n",
    "    super(CustomModel,self).__init__() \n",
    "    self.num_labels = num_labels \n",
    "\n",
    "    self.model = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "    self.dropout = nn.Dropout(0.1) \n",
    "    self.classifier = nn.Linear(768,21) \n",
    "\n",
    "  def forward(self, input_ids=None, attention_mask=None,labels=None):\n",
    "    #utiliza el modelo para generar la salida\n",
    "    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    #aplica el resto de capas\n",
    "    sequence_output = self.dropout(outputs[0]) #outputs[0]=último estado\n",
    "    logits = self.classifier(sequence_output[:,0,:].view(-1,768)) # calcula el error\n",
    "    \n",
    "    loss = None\n",
    "    if labels is not None:\n",
    "      loss_fct = nn.CrossEntropyLoss()\n",
    "      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "    \n",
    "    return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=CustomModel(checkpoint= model_name, num_labels=21).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(encoded_dataset[\"train\"], shuffle=True, batch_size=32, collate_fn=data_collator)\n",
    "\n",
    "eval_dataloader = DataLoader(encoded_dataset[\"validation\"], batch_size=32, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "import evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = evaluate.load(\"f1\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar_train = tqdm(range(num_training_steps))\n",
    "progress_bar_eval = tqdm(range(num_epochs * len(eval_dataloader)))\n",
    "\n",
    "best_loss = float('inf')  # Inicializa con infinito\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  for batch in train_dataloader:\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "      batch[\"labels\"] = batch[\"labels\"] - 1\n",
    "      outputs = model(**batch)\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      lr_scheduler.step()\n",
    "      optimizer.zero_grad()\n",
    "      progress_bar_train.update(1)\n",
    "\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  num_batches = 0\n",
    "  for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    batch[\"labels\"] = batch[\"labels\"] - 1\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    f1.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    precision.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    recall.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    progress_bar_eval.update(1)\n",
    "    total_loss += outputs.loss.item()\n",
    "    num_batches += 1\n",
    "  \n",
    "  avg_loss = total_loss / num_batches\n",
    "  print(avg_loss)\n",
    "  print(f1.compute(average='micro'))\n",
    "  print(precision.compute(average='micro'))\n",
    "  print(recall.compute(average='micro'))\n",
    "\n",
    "  if avg_loss < best_loss:\n",
    "    best_loss = avg_loss\n",
    "    torch.save(model.state_dict(), \"model_state.bin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
