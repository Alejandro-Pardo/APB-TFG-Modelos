{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.read_csv('T1/train/g_qrels_majority_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows: \", dt.shape[0])\n",
    "print(\"Number of columns: \", dt.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = dt['rel']\n",
    "\n",
    "# Crear el histograma\n",
    "counts, bins, patches = plt.hist(rel, bins=[-0.5, 0.5, 1.5], edgecolor='white')\n",
    "\n",
    "# Centrar las etiquetas en las barras\n",
    "plt.xticks([0, 1])\n",
    "\n",
    "# Mostrar el valor de cada barra\n",
    "for count, bin, patch in zip(counts, bins, patches):\n",
    "    plt.text(bin+0.5, count+0.5, int(count), ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel('Relevancia')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see the distribution of the majority class in the training set where the relevance score is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_filtered = dt[dt['rel'] == 1]\n",
    "\n",
    "# Obtener la columna 'query' del DataFrame filtrado\n",
    "query = dt_filtered['query']\n",
    "\n",
    "# Crear el histograma\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "counts, bins, patches = plt.hist(query, bins=range(1, 23), edgecolor='white', rwidth=0.7)\n",
    "\n",
    "# Centrar las etiquetas en las barras\n",
    "plt.xticks(np.arange(1.5, 22.5, 1), range(1, 22))\n",
    "\n",
    "# Mostrar el valor de cada barra\n",
    "for count, bin, patch in zip(counts, bins, patches):\n",
    "    plt.text(bin+0.5, count+0.5, int(count), ha='center'  , va='bottom')\n",
    "\n",
    "plt.xlabel('Síntoma')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unanimity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt2 = pd.read_csv('T1/train/g_rels_consenso.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows: \", dt2.shape[0])\n",
    "print(\"Number of columns: \", dt2.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel = dt2['rel']\n",
    "\n",
    "# Crear el histograma\n",
    "counts, bins, patches = plt.hist(rel, bins=[-0.5, 0.5, 1.5], edgecolor='white')\n",
    "\n",
    "# Centrar las etiquetas en las barras\n",
    "plt.xticks([0, 1])\n",
    "\n",
    "# Mostrar el valor de cada barra\n",
    "for count, bin, patch in zip(counts, bins, patches):\n",
    "    plt.text(bin+0.5, count+0.5, int(count), ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel('Relevancia')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see the distribution of the unanimity class in the training set where the relevance score is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_filtered2 = dt2[dt2['rel'] == 1]\n",
    "\n",
    "# Obtener la columna 'query' del DataFrame filtrado\n",
    "query = dt_filtered2['query']\n",
    "\n",
    "# Crear el histograma\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "counts, bins, patches = plt.hist(query, bins=range(1, 23), edgecolor='white', rwidth=0.7)\n",
    "\n",
    "# Centrar las etiquetas en las barras\n",
    "plt.xticks(np.arange(1.5, 22.5, 1), range(1, 22))\n",
    "\n",
    "# Mostrar el valor de cada barra\n",
    "for count, bin, patch in zip(counts, bins, patches):\n",
    "    plt.text(bin+0.5, count+0.5, int(count), ha='center'  , va='bottom')\n",
    "\n",
    "plt.xlabel('Síntoma')\n",
    "plt.ylabel('Frecuencia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between majority and unanimity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rel1 = dt['rel'].value_counts()\n",
    "rel2 = dt2['rel'].value_counts()\n",
    "\n",
    "diff = rel2 - rel1\n",
    "\n",
    "diff_df = pd.DataFrame({'Rel': diff.index, 'Diferencia': diff.values})\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "barplot = sns.barplot(x='Rel', y='Diferencia', data=diff_df, hue='Rel', palette='pastel', legend=False)\n",
    "\n",
    "plt.xlabel('Relevancia')\n",
    "plt.ylabel('Diferencia')\n",
    "\n",
    "for p in barplot.patches:\n",
    "    barplot.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                     ha='center', va='top', xytext=(0, 10), textcoords='offset points')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'T1/train/data/'\n",
    "total_posts = 0\n",
    "total_phrases = 0\n",
    "total_users = 0\n",
    "\n",
    "# Recorrer todos los archivos en el directorio\n",
    "for path in os.listdir(directory):\n",
    "    filename = os.path.join(directory, path)\n",
    "    total_users += 1\n",
    "    with open(filename, 'r', encoding='utf8' ) as file:\n",
    "        content = '<ROOT>' + file.read() + '</ROOT>'\n",
    "    root = ET.fromstring(content)\n",
    "    last_post_id = None\n",
    "    for doc in root.findall('DOC'):\n",
    "        post_id = doc.find('DOCNO').text.split('_')[2]\n",
    "        if post_id != last_post_id:\n",
    "            total_posts += 1\n",
    "            last_post_id = post_id\n",
    "        total_phrases += 1\n",
    "\n",
    "# Calcular medias\n",
    "average_posts_per_user = total_posts / total_users\n",
    "average_phrases_per_post = total_phrases / total_posts\n",
    "average_phrases_per_user = total_phrases / total_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Número total de usuarios: {total_users}')\n",
    "print(f'Número total de frases: {total_phrases}')\n",
    "print(f'Número medio de posts por usuario: {average_posts_per_user}')\n",
    "print(f'Número medio de frases por post: {average_phrases_per_post}')\n",
    "print(f'Número medio de frases totales por usuario: {average_phrases_per_user}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Size Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear un dataset con las oraciones que son relevantes y vamos a estudiar la distribucion de la longitud de las oraciones para cada sintoma, para el dataset de mayoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.read_csv('T1/train/g_qrels_majority_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant = dt[dt['rel'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'symptom': relevant['query'], 'docid': relevant['docid'], 'text': ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "docid_dict = defaultdict(list)\n",
    "for i, docid in enumerate(data['docid']):\n",
    "    docid_dict[docid].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'T1/train/data/'\n",
    "# Recorrer todos los archivos en el directorio\n",
    "for path in os.listdir(directory):\n",
    "    filename = os.path.join(directory, path)\n",
    "    with open(filename, 'r', encoding='utf8') as file:\n",
    "        try:\n",
    "            content = '<ROOT>' + file.read() + '</ROOT>'\n",
    "            root = ET.fromstring(content)\n",
    "            for doc in root.findall('DOC'):\n",
    "                docid = doc.find('DOCNO').text\n",
    "                text = str(doc.find('TEXT').text).strip()\n",
    "                #data.loc[data['docid'] == docid, 'text'] = text\n",
    "                if docid in docid_dict:\n",
    "                    for i in docid_dict[docid]:\n",
    "                        data.loc[i, 'text'] = text\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"length_text\"]=data['text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['length_text'].describe(percentiles=[0, 0.25, 0.50, 0.75, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a mostrar los datos de manera más visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['plum', 'violet', 'red', 'coral', 'darksalmon', 'olivedrab', 'yellowgreen', 'darkseagreen', 'lightgreen', 'steelblue', 'skyblue', 'navy', 'blue', 'purple', 'magenta', 'pink', 'crimson', 'orange', 'gold', 'yellow', 'lime']\n",
    "labels = sorted(data['symptom'].unique().tolist())\n",
    "#print(labels)\n",
    "dict_color = dict(zip(labels, colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.hist(data['length_text'],  color = 'purple', edgecolor = 'white')\n",
    "ax.set_xlabel('Número de tokens', fontsize=14)\n",
    "ax.set_ylabel('Número de textos', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_size = (20, 10)\n",
    "plt.figure(figsize=fig_size)\n",
    "labels = sorted(data[\"symptom\"].unique())\n",
    "for name in labels:\n",
    "    # Subset to the language\n",
    "    subset = data[data['symptom'] == int(name)]\n",
    "    # Draw the density plot\n",
    "    sns.kdeplot(subset['length_text'], color=dict_color[name], label=name, legend=True)\n",
    "\n",
    "plt.xlabel('Número de tokens del texto original', fontsize=14 )\n",
    "plt.ylabel('Densidad', fontsize=14)\n",
    "plt.legend(prop={'size': 15}, title='Síntomas', loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, esto lo hemos hecho con el método split(), que no es lo más preciso. Por eso, ahora utilizaremos la libreria spacy para tokenizar las oraciones y así obtener el número de tokens de manera más precisa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "#python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nlp.Defaults.stop_words\n",
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() for word in doc ]\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "    sentence = \" \".join(mytokens)\n",
    "    # return preprocessed list of tokens\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['text'].apply(spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"length_tokens\"]=data['tokens'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['length_tokens'].describe(percentiles=[0, 0.25, 0.50, 0.75, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.hist(data['length_tokens'],  color = 'purple', edgecolor = 'white')\n",
    "ax.set_xlabel('Número de tokens', fontsize=14)\n",
    "ax.set_ylabel('Número de textos', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_size = (20, 10)\n",
    "plt.figure(figsize=fig_size)\n",
    "labels = sorted(data[\"symptom\"].unique())\n",
    "for name in labels:\n",
    "    # Subset to the language\n",
    "    subset = data[data['symptom'] == int(name)]\n",
    "    # Draw the density plot\n",
    "    sns.kdeplot(subset['length_tokens'], color=dict_color[name], label=name, legend=True)\n",
    "\n",
    "plt.xlabel('Número de tokens del texto procesado por Spacy', fontsize=14 )\n",
    "plt.ylabel('Densidad', fontsize=14)\n",
    "plt.legend(prop={'size': 15}, title='Síntomas', loc=\"upper right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('T1/train/relevant_texts.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
